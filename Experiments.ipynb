{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Smoothed Duality Gap as a Stopping Criterion\n",
    "\n",
    "This notebook provides the different experiments conducted in order to validate and illustrate our theoretical findings outlined in our paper entitled: ***The Smoothed Duality Gap as a Stopping Criterion***.\n",
    "\n",
    "**- Abstract:** we optimize the running time of the primal-dual algorithms by optimizing their stopping criteria for solving convex optimization problems under affine equality constraints, which means terminating the algorithm earlier with fewer iterations. We study the relations between four stopping criteria and show under which conditions they are accurate to detect optimal solutions. The uncomputable one: *\"Optimality gap and Feasibility error\"*, and the computable ones: the *\"Karush-Kuhn-Tucker error\"*, the *\"Projected Duality Gap\"*, and the *\"Smoothed Duality Gap\"*. Assuming metric sub-regularity or quadratic error bound, we establish that all of the computable criteria provide practical upper bounds for the optimality gap, and approximate it effectively. Furthermore, we establish comparability between some of the computable criteria under certain conditions. Numerical experiments on basis pursuit, and quadratic programs with(out) non-negative weights corroborate these findings and show the superior stability of the smoothed duality gap over the rest.\n",
    "\n",
    "**- Framework of Interest:** we are interested in convex optimization problems under affine equality constraints. That is:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\min_{x \\in \\mathcal{X}} ~ f(x) \\hspace{1cm} \\text{subject to} \\hspace{1cm} Ax = b\n",
    "\\end{equation}\n",
    "\n",
    "where $f \\colon \\mathcal{X} \\rightarrow \\mathbb{R} \\cup \\{+\\infty\\}$ is a proper, lower semi-continuous, and convex function with a computable proximal operator, $A \\colon \\mathcal{X} \\rightarrow \\mathcal{Y}$ is a linear operator, and $b \\in \\mathcal{Y}$. We call it the ***primal problem***\n",
    "\n",
    "**- Employed Algorithm:** we employ the ***Primal-Dual Hybrid Gradient (PDHG)*** algorithm to solve the associated ***saddle point problem***: \n",
    "\\begin{equation}\n",
    "    \\min_{x \\in \\mathcal{X}} \\max_{y \\in \\mathcal{Y}} ~ \\mathcal{L}(x, y) := f(x) + \\left\\langle Ax - b, y \\right\\rangle \n",
    "\\end{equation}\n",
    "where $\\mathcal{L}(x, y)$ is the ***associated Lagrangian*** with the primal problem and $y \\in \\mathcal{Y}$ is the so-called Lagrange multiplier or dual variable.\n",
    "\n",
    "<center>\n",
    "\n",
    "|  Algorithm: Primal-Dual Hybrid Gradient (PDHG) |\n",
    "|-----------------------------------------------|\n",
    "| $\\hspace{1.25cm}\\begin{aligned} \\bar{x}_{k+1} &= \\mathrm{Prox}_{\\tau f}\\left(x_k - \\tau A^Ty_k\\right) \\\\ \\bar{y}_{k+1} &= y_k + \\sigma \\left(A\\bar{x}_{k+1} - b\\right)\\\\ x_{k+1} &= \\bar{x}_{k+1} - \\tau A^T\\left(\\bar{y}_{k+1} - y_k\\right)  \\\\ y_{k+1} &= \\bar{y}_{k+1} \\\\ \\end{aligned}$| \n",
    "\n",
    "</center>\n",
    "\n",
    "with primal and dual step sizes: $\\tau = \\frac{0.95}{\\|A\\|}$ and $\\sigma = \\frac{1}{\\|A\\|}$, respectively. \n",
    "\n",
    " \n",
    "**- Conducted Experiments:**\n",
    "* *Linearly-constrained Least-Squares (LC-LS)*.\n",
    "  * *One-dimensional* problem. \n",
    "  * *I.I.D. Gaussian matrices* problem. \n",
    "  * *Non-trivial covariance matrices* problem. \n",
    "  * *Distributed optimization* problem. \n",
    "* *Quadratic Programming (QP)*. \n",
    "*  *Basis Pursuit (BP)*.\n",
    "\n",
    "**- For each experiment, we do the following:**\n",
    "* We find the optimal solution of the problem either analytically or by using CVXPY.\n",
    "* We solve the problem using ***(PDHG)***. \n",
    "* We Computed all the measures, presented in Sections 3 and 4: \n",
    "  * *Optimality Gap (OG).*\n",
    "  * *Feasibility Error (FE).* \n",
    "  * *Karush–Kuhn–Tucker (KKT) error.* \n",
    "  * *Projected Duality Gap (PDG).*\n",
    "  * *Smoothed Duality Gap (SDG).*\n",
    "* We compute the optimality gap bounds (approximations), presented in Section 5. \n",
    "* We compute the comparability bounds, presented in Section 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "from PDHG import PDHG\n",
    "from OptimizationBounds import OptimizationBounds, find_iter, avg_sd\n",
    "from Plotter import Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OG_all, comparability_all = {}, {}\n",
    "\n",
    "GK = \"G < K\"\n",
    "KG = \"K < G\"\n",
    "GD = \"G < D\"\n",
    "DG = \"D < G\"\n",
    "comparability = {GK: None, KG: None, GD: None, DG: None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly-Constrained Least-Squares (LC-LS)\n",
    "\n",
    "Our first class of experiments is the ***Linearly-Constrained Least-Squares (LC-LS)*** class presented in Sub-section **7.1**. \n",
    "\n",
    "\\begin{equation} \n",
    "    \\min_{x \\in \\mathbb{R}^n} ~  \\frac{1}{2} \\|Qx - c\\|^2   \\hspace{1cm} \\text{subject to} \\hspace{1cm} Ax = b\n",
    "\\end{equation}\n",
    "\n",
    "We conduct several instances of this class: \n",
    "  - One-dimensional.\n",
    "  - I.I.D. Gaussian matrices.\n",
    "  - Non-trivial covariance matrices.\n",
    "  - Distributed optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: One-dimensional \n",
    "\n",
    "We start by the simple instance presented in sub-subsection **7.1.2** where we can, analytically, find the exact optimal solution, $x^{\\star}$. \n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\min_{x \\in \\mathbb{R}} ~ \\frac{1}{2} &\\left(\\frac{1}{9}x - 2\\right)^2 \\\\ \n",
    "        &~~~~ 9x = 7\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "The *primal-feasibility* condition within the KKT conditions implies that $x^{\\star} = \\frac{7}{9}$. Consequently, in this particular problem, we can gain a more precise assessment of the quality of our approximations for the *optimality gap*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OptimizationProblems import OneDim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "n, m = 1, 1\n",
    "Q, c, A, b = np.array([[1/9]]), np.array([2]), np.array([[9]]), np.array([7])\n",
    "\n",
    "# Problem initialization.\n",
    "prob = OneDim(n, m, Q, c, A, b)\n",
    "x_star, f_star, L, gamma = prob.prob_parameters(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDHG solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values \n",
    "xo = np.zeros(n)\n",
    "yo = np.random.rand(m)\n",
    "\n",
    "Alg = PDHG(xo, yo, prob.prox_LS, A, b, x_star)\n",
    "result = Alg.two_runs(stop_crt_kwargs={'f':prob.LS, 'eta_fun':prob.eta_QEBSG})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = OptimizationBounds(result[\"Primal variables\"], result[\"Dual variables\"], prob.LS, prob.prox_LS, \n",
    "            prob.stationarity, prob.LSc, prob.proj_LSc, f_star, x_star, A, b, \n",
    "            L = L, Lc_grad= prob.gradc_Lipschitz(), eta_fun= prob.ETA_BETAs, gamma = gamma)\n",
    "\n",
    "OG, FG, KKT, PDG, SDG = bounds.OG, bounds.FG, bounds.KKT, bounds.PDG, bounds.SDG[:, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OG_bounds, KKT_SDG_bounds, PDG_SDG_bounds = bounds.all_bounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'One-dimensional'\n",
    "OG_all[exp_name] = (OG_bounds, result['step'])\n",
    "\n",
    "comparability = {GK: [KKT_SDG_bounds['G<K']['SDG'], KKT_SDG_bounds['G<K']['bound']],\n",
    "                 KG: [KKT, KKT_SDG_bounds['K<G']],\n",
    "                 GD: [PDG_SDG_bounds['G<D']['SDG'], PDG_SDG_bounds['G<D']['bound']],\n",
    "                 DG: [PDG, PDG_SDG_bounds['D<G']]}\n",
    "comparability_all[exp_name] = comparability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = Plotter()\n",
    "    \n",
    "# plots.findings_plot(OG, FG, KKT, PDG, SDG, OG_bounds, KKT_SDG_bounds, PDG_SDG_bounds, result['step'])\n",
    "plots.plot_all(paper=True, prob=exp_name, OG=OG, FG=FG, KKT=KKT, PDG = PDG, SDG = SDG, OG_bounds=OG_bounds, KKT_SDG_bounds=KKT_SDG_bounds, PDG_SDG_bounds=PDG_SDG_bounds, step=result['step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: I.I.D. Gaussian matrices \n",
    "\n",
    "We examine the ***LC-LS*** problem with dimensions set to $n = 20$ and $m = 10$. In this scenario, we generate independently and identically distributed (i.i.d.) Gaussian matrices $Q$ and $A$, as presented in sub-subsection **7.1.2**. That is:\n",
    "\n",
    "\\begin{equation} \n",
    "\\begin{split}    \n",
    "Q \\in \\mathbb{R}^{m \\times n} ~\\text{s.t.} ~\\forall (i, j) \\in \\{ 1, \\dots, m \\} \\times \\{ 1, \\dots, n \\}, ~ Q_{ij} \\sim \\mathcal{N}(0, 1) ~ i.i.d. \\\\ \n",
    "    A \\in \\mathbb{R}^{m \\times n} ~\\text{s.t.} ~\\forall (i, j) \\in \\{ 1, \\dots, m \\} \\times \\{ 1, \\dots, n \\}, ~ A_{ij} \\sim \\mathcal{N}(0, 1) ~ i.i.d. \n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OptimizationProblems import LeastSquares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random data.\n",
    "n, m = 20, 10\n",
    "np.random.seed(2)\n",
    "Q = np.random.randn(m, n)\n",
    "c = np.random.randn(m)\n",
    "A = np.random.randn(m, n)\n",
    "b = np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem initialization.\n",
    "prob = LeastSquares(n, m, Q, c, A, b)\n",
    "x_star, f_star, L, gamma = prob.prob_parameters(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDHG solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values \n",
    "xo = np.zeros(n)\n",
    "yo = np.random.rand(m)\n",
    "\n",
    "Alg = PDHG(xo, yo, prob.prox_LS, A, b, x_star)\n",
    "result = Alg.two_runs(stop_crt_kwargs={'f':prob.LS, 'eta_fun':prob.eta_QEBSG})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measures and bounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = OptimizationBounds(result[\"Primal variables\"], result[\"Dual variables\"], prob.LS, prob.prox_LS, \n",
    "            prob.stationarity, prob.LSc, prob.proj_LSc, f_star, x_star, A, b, \n",
    "            L = L, Lc_grad= prob.gradc_Lipschitz(), eta_fun= prob.ETA_BETAs, gamma = gamma)\n",
    "\n",
    "OG, FG, KKT, PDG, SDG = bounds.OG, bounds.FG, bounds.KKT, bounds.PDG, bounds.SDG[:, -1]\n",
    "OG_bounds, KKT_SDG_bounds, PDG_SDG_bounds = bounds.all_bounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'I.I.D. Gaussian matrices'\n",
    "OG_all[exp_name] = (OG_bounds, result['step'])\n",
    "\n",
    "comparability = {GK: [KKT_SDG_bounds['G<K']['SDG'], KKT_SDG_bounds['G<K']['bound']],\n",
    "                 KG: [KKT, KKT_SDG_bounds['K<G']],\n",
    "                 GD: [PDG_SDG_bounds['G<D']['SDG'], PDG_SDG_bounds['G<D']['bound']],\n",
    "                 DG: [PDG, PDG_SDG_bounds['D<G']]}\n",
    "comparability_all[exp_name] = comparability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = Plotter()\n",
    "# plots.findings_plot(OG, FG, KKT, PDG, SDG, OG_bounds, KKT_SDG_bounds, PDG_SDG_bounds, result['step'])\n",
    "plots.plot_all(paper=False,prob=exp_name, OG=OG, FG=FG, KKT=KKT, PDG = PDG, SDG = SDG, OG_bounds=OG_bounds, KKT_SDG_bounds=KKT_SDG_bounds, PDG_SDG_bounds=PDG_SDG_bounds, step=result['step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Non-trivial covariance \n",
    "\n",
    "We investigate the ***LC-LS*** problem with dimensions set to $n = 20$ and $m = 10$. In this instance, we generate matrices $Q$ and $A$ with non-trivial covariance matrices, as presented in sub-subsection **7.1.2**, defined as follows:\n",
    "\n",
    "\\begin{align} \n",
    "        A = \\Sigma_a X_a \\hspace{0.75cm} && \\hspace{0.75cm} Q = \\Sigma_q X_q \n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "* The matrices $X_a, X_q \\in \\mathbb{R}^{m \\times n}$ are Gaussian matrices, following the pattern established in the previous case.\n",
    "* The matrices $\\Sigma_a, \\Sigma_q \\in \\mathbb{R}^{m \\times m}$ serve as covariance matrices, generated using the Python built-in function [`toeplitz`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.toeplitz.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import sqrtm, toeplitz\n",
    "from OptimizationProblems import LeastSquares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Non-Trivial Covariance random matrices\n",
    "n, m = 20, 10\n",
    "# np.random.seed(3)\n",
    "c = np.random.randn(m)\n",
    "Q = sqrtm(toeplitz(0.5**np.arange(m))) @ np.random.rand(m, n)\n",
    "A = sqrtm(toeplitz(0.3**np.arange(m))) @ np.random.rand(m, n)\n",
    "b = np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem initialization.\n",
    "prob = LeastSquares(n, m, Q, c, A, b)\n",
    "x_star, f_star, L, gamma = prob.prob_parameters(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDHG solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values \n",
    "xo = np.zeros(n)\n",
    "yo = np.random.rand(m)\n",
    "\n",
    "Alg = PDHG(xo, yo, prob.prox_LS, A, b, x_star)\n",
    "# result = Alg.two_runs(stop_crt_kwargs={'f':prob.LS, 'eta_fun':prob.eta_QEBSG})\n",
    "result = Alg.two_runs(stop_crt = 'KKT error', stop_crt_kwargs={'f': prob.LS, 'stationarity': prob.stationarity, 'gamma': prob.gamma_MSR()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measures and bounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = OptimizationBounds(result[\"Primal variables\"], result[\"Dual variables\"], prob.LS, prob.prox_LS, \n",
    "            prob.stationarity, prob.LSc, prob.proj_LSc, f_star, x_star, A, b, \n",
    "            L = L, Lc_grad= prob.gradc_Lipschitz(), eta_fun= prob.ETA_BETAs, gamma = gamma)\n",
    "\n",
    "OG, FG, KKT, PDG, SDG = bounds.OG, bounds.FG, bounds.KKT, bounds.PDG, bounds.SDG[:, -1]\n",
    "OG_bounds, KKT_SDG_bounds, PDG_SDG_bounds = bounds.all_bounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'Non-trivial covariance'\n",
    "OG_all[exp_name] = (OG_bounds, result['step'])\n",
    "\n",
    "comparability = {GK: [KKT_SDG_bounds['G<K']['SDG'], KKT_SDG_bounds['G<K']['bound']],\n",
    "                 KG: [KKT, KKT_SDG_bounds['K<G']],\n",
    "                 GD: [PDG_SDG_bounds['G<D']['SDG'], PDG_SDG_bounds['G<D']['bound']],\n",
    "                 DG: [PDG, PDG_SDG_bounds['D<G']]}\n",
    "comparability_all[exp_name] = comparability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = Plotter()\n",
    "# plots.findings_plot(OG, FG, KKT, PDG, SDG, OG_bounds, KKT_SDG_bounds, PDG_SDG_bounds, result['step'])\n",
    "plots.plot_all(prob=exp_name, OG=OG, FG=FG, KKT=KKT, PDG = PDG, SDG = SDG, OG_bounds=OG_bounds, KKT_SDG_bounds=KKT_SDG_bounds, PDG_SDG_bounds=PDG_SDG_bounds, step=result['step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: Distributed optimization\n",
    "\n",
    "We examine the unconstrained optimization problem presented as follows:\n",
    "\n",
    "\\begin{equation} \n",
    "    \\min_{x \\in \\mathbb{R}^n} ~ \\frac{1}{2} \\sum_{i = 1}^M  \\left\\| Q_ix - c_i\\right\\|^2 \n",
    "\\end{equation}\n",
    "\n",
    "- In this formulation, for each $i \\in \\llbracket 1, M \\rrbracket $, the matrix $Q_i \\in \\mathbb{R}^{m \\times n}$ and the vector $c_i \\in \\mathbb{R}^m$ are real data sourced from the [`bodyfat`](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html) dataset. This data comprises 252 data-points, each characterized by 14 features. In our case, we divide the dataset into $M = 3, n = 14$, and $m = 84$.\n",
    "\n",
    "- As presented in Sub-section **7.2**, we can re-formulate the unconstrained problem into a constrained one: \n",
    "\n",
    "    \\begin{equation} \n",
    "        \\min_{X \\in \\mathbb{R}^{Mn}} ~ \\frac{1}{2} \\left\\| \\mathbf{Q} X  - \\mathbf{c} \\right\\|^2 \\hspace{1cm} \\text{subject to} \\hspace{1cm} AX = 0\n",
    "    \\end{equation}\n",
    "\n",
    "    where, $X_{Mn} := \\begin{bmatrix} x_1 & \\dots & x_M \\end{bmatrix}^T$, $\\mathbf{c}_{Mm} := \\begin{bmatrix} c_1 & \\dots & c_M \\end{bmatrix}^T$, and \n",
    "\n",
    "    \\begin{align*}\n",
    "        \\mathbf{Q}_{Mm \\times Mn} := \\begin{bmatrix}\n",
    "            Q_1 & \\mathbf{0}_{m \\times n} & \\dots & \\mathbf{0}_{m \\times n} \\\\ \\mathbf{0}_{m \\times n} & Q_2 & \\dots & \\mathbf{0}_{m \\times n} \\\\ \\vdots & \\dots & \\ddots & \\dots \\\\ \\mathbf{0}_{m \\times n} & \\dots & \\mathbf{0}_{m \\times n} & Q_M\n",
    "        \\end{bmatrix} && A_{(M-1)n \\times Mn} := \\begin{bmatrix}\n",
    "            \\mathbf{Id}_n & -\\mathbf{Id}_n & \\mathbf{0}_n & \\dots & \\mathbf{0}_n \\\\ \n",
    "            \\mathbf{0}_n  & \\mathbf{Id}_n & -\\mathbf{Id}_n  & \\dots & \\mathbf{0}_n \\\\\n",
    "            \\vdots & \\dots & \\ddots & \\ddots & \\dots\\\\\n",
    "            \\mathbf{0}_n & \\dots &  \\mathbf{0}_n & \\mathbf{Id}_n & -\\mathbf{Id}_n \n",
    "        \\end{bmatrix} \n",
    "    \\end{align*}\n",
    "\n",
    "- Moreover, thanks to the unconstrained problem, the minimizer $x^{\\star}$ can be easily found: \n",
    "\n",
    "\\begin{equation}  \\left(\\sum_{i = 1}^M Q^T_iQ_i\\right)x^{\\star} = \\sum_{i = 1}^M Q^T_i c_i  \\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OptimizationProblems import DistributedOPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "M, nbdata, nbfeatures = 3, 252, 14\n",
    "data = 'bodyfat_scale.txt'\n",
    "\n",
    "# Problem initialization.\n",
    "prob = DistributedOPT(data, M, nbdata, nbfeatures)\n",
    "x_star, f_star, L, gamma = prob.prob_parameters(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDHG solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values \n",
    "xo = np.random.rand(M*nbfeatures)\n",
    "yo = np.random.rand((M-1)*nbfeatures)\n",
    "\n",
    "Alg = PDHG(xo, yo, prob.prox_LS, prob.A, prob.b, x_star)\n",
    "result = Alg.two_runs(stop_crt = 'KKT error', stop_crt_kwargs={'f': prob.LS, 'stationarity': prob.stationarity, 'gamma': prob.gamma_MSR()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measures and bounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = OptimizationBounds(result[\"Primal variables\"], result[\"Dual variables\"], prob.LS, prob.prox_LS, \n",
    "            prob.stationarity, prob.LSc, prob.proj_LSc, f_star, x_star, prob.A, prob.b, \n",
    "            L = L, Lc_grad= prob.gradc_Lipschitz(), eta_fun= prob.ETA_BETAs, gamma = gamma)\n",
    "\n",
    "OG, FG, KKT, PDG, SDG = bounds.OG, bounds.FG, bounds.KKT, bounds.PDG, bounds.SDG[:, -1]\n",
    "OG_bounds, KKT_SDG_bounds, PDG_SDG_bounds = bounds.all_bounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'Distributed optimization'\n",
    "OG_all[exp_name] = (OG_bounds, result['step'])\n",
    "\n",
    "comparability = {GK: [KKT_SDG_bounds['G<K']['SDG'], KKT_SDG_bounds['G<K']['bound']],\n",
    "                 KG: [KKT, KKT_SDG_bounds['K<G']],\n",
    "                 GD: [PDG_SDG_bounds['G<D']['SDG'], PDG_SDG_bounds['G<D']['bound']],\n",
    "                 DG: [PDG, PDG_SDG_bounds['D<G']]}\n",
    "comparability_all[exp_name] = comparability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = Plotter()\n",
    "# plots.findings_plot(OG, FG, KKT, PDG, SDG, OG_bounds, KKT_SDG_bounds, PDG_SDG_bounds, result['step'])\n",
    "plots.plot_all(paper=False, prob=exp_name, OG=OG, FG=FG, KKT=KKT, PDG = PDG, SDG = SDG, OG_bounds=OG_bounds, KKT_SDG_bounds=KKT_SDG_bounds, PDG_SDG_bounds=PDG_SDG_bounds, step=result['step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Programming (QP)\n",
    "\n",
    "As we have mentioned, our theoretical findings have primarily focused on optimization problems under equality affine constraints. However, in this experiment, we aim to extend the applicability of our theoretical insights to address optimization problems that incorporate inequality constraints. Therefore, we consider the same **LC-LS** problem as discussed earlier, but now incorporating the additional requirement of non-negativity constraints on the weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5\n",
    "\n",
    "We consider the following *Quadratic Programming* problem: \n",
    "\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "    \\min_{x \\in \\mathbb{R}^n}  \\frac{1}{2} & \\|Qx - c\\|^2 \\\\ \n",
    "    \\text{s.t.} ~~ & Ax = b \\\\ \n",
    "     & \\hspace{0.3cm} x \\geq 0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "In Sub-section **7.3**, we show how one can re-formulate this problem to project it into our framework of interest. In short, we first encapsulate the non-negativity constraint by introducing an indicator function withing the objective function. Subsequently, we introduce an additional variable, $\\tilde{x}$, and incorporate an extra constraint, $x = \\tilde{x}$. That is: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\min_{X \\in \\mathbb{R}^{2n}}  F(X) = \\frac{1}{2}  \\|Qx - c\\|^2 & + \\imath_{\\mathbb{R}_+^n}(\\tilde{x})\\\\ \n",
    "    \\text{subject to} \\hspace{1cm}  \\tilde{A}X = B& \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "where \n",
    "\\begin{align}\n",
    "    X := \\begin{bmatrix}\n",
    "        x \\\\ \\tilde{x}\n",
    "    \\end{bmatrix} && \\tilde{A} := \\begin{bmatrix}\n",
    "        A & 0 \\\\ \\mathbf{Id} & - \\mathbf{Id} \n",
    "    \\end{bmatrix} && B := \\begin{bmatrix}\n",
    "        b \\\\ 0\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "This re-formulation projects the problem into our framework, and would render the computation more manageable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import sqrtm, toeplitz\n",
    "from PDHG import PDHG3\n",
    "from OptimizationProblems import QuadraticProgramming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Non-Trivial Covariance random matrices\n",
    "n, m = 20, 10\n",
    "np.random.seed(5)\n",
    "Q = sqrtm(toeplitz(0.5**np.arange(m))) @ np.random.rand(m, n)\n",
    "c = np.random.randn(m)\n",
    "A = sqrtm(toeplitz(0.3**np.arange(m))) @ np.random.rand(m, n)\n",
    "b = np.zeros(m)\n",
    "\n",
    "A_tilde = np.block([[A, np.zeros((m , n))], [np.eye(n), -1*np.eye(n)]])\n",
    "B = np.hstack((b, np.zeros(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem initialization.\n",
    "prob = QuadraticProgramming(n, m, Q, c, A_tilde, B)\n",
    "x_star, f_star, L, gamma = prob.prob_parameters(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDHG solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values \n",
    "xo = np.random.rand(2*n)\n",
    "yo = np.random.rand(n + m)\n",
    "\n",
    "Alg = PDHG3(xo, yo, prob.prox_objective, A_tilde, B, x_star = np.tile(x_star, 2))\n",
    "result = Alg.two_runs(stop_crt_kwargs={'f':prob.objective, 'eta_fun':prob.eta_QEBSG})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measures and bounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = OptimizationBounds(result[\"Primal bar variables\"], result[\"Dual variables\"], prob.objective, prob.prox_objective, \n",
    "            prob.stationarity, prob.objective_c, prob.proj_objective_c, f_star, x_star, A_tilde, B, \n",
    "            L = L, Lc_grad= prob.gradc_Lipschitz(), eta_fun= prob.ETA_BETAs, gamma = gamma)\n",
    "\n",
    "OG, FG, KKT, PDG, SDG = bounds.OG, bounds.FG, bounds.KKT, bounds.PDG, bounds.SDG[:, 0]\n",
    "OG_bounds, KKT_SDG_bounds, PDG_SDG_bounds = bounds.all_bounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'Quadratic programming'\n",
    "OG_all[exp_name] = (OG_bounds, result['step'])\n",
    "\n",
    "comparability = {GK: [KKT_SDG_bounds['G<K']['SDG'], KKT_SDG_bounds['G<K']['bound']],\n",
    "                 KG: [KKT, KKT_SDG_bounds['K<G']],\n",
    "                 GD: [PDG_SDG_bounds['G<D']['SDG'], PDG_SDG_bounds['G<D']['bound']],\n",
    "                 DG: [PDG, PDG_SDG_bounds['D<G']]}\n",
    "comparability_all[exp_name] = comparability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = Plotter()\n",
    "# plots.findings_plot(OG, FG, KKT, PDG, SDG, OG_bounds, KKT_SDG_bounds, PDG_SDG_bounds, result['step'])\n",
    "plots.plot_all(paper=False,prob=exp_name, OG=OG, FG=FG, KKT=KKT, PDG = PDG, SDG = SDG, OG_bounds=OG_bounds, KKT_SDG_bounds=KKT_SDG_bounds, PDG_SDG_bounds=PDG_SDG_bounds, step=result['step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis pursuit\n",
    "\n",
    "Our last experiment, as presented in Sub-section **7.4**, involves a non-smooth convex minimization problem known as the ***Basis Pursuit*** problem. It aims to minimize the $\\ell_1$ norm while satisfying a system of linear equation.\n",
    "\n",
    "The primary objective of this experiment is to highlight the superior stability of the *Smoothed duality Gap* compared to the *Karush–Kuhn–Tucker error*, as will be demonstrated subsequently.\n",
    "\n",
    "This experiment shows that the *KKT error* is **algorithm-dependant**. You will see that the KKT error will not converge when employing the PDHG algorithm presented above to compute the KKT error. However, as explained in Sub-section **7.4**, if we employ a different version of PDHG, it does converge. \n",
    "\n",
    "Here are the two versions of PDHG: \n",
    "\n",
    "<center>\n",
    "\n",
    "|        Version 1       |      Interpretation       |\n",
    "|:----------------------:|:-------------------------:|\n",
    "|$\\bar{x}_{k+1} = \\mathrm{prox}_{\\tau f}\\left(x_k - \\tau A^Ty_k\\right)$ |Primal Forward-Backward step   |\n",
    "|$\\bar{y}_{k+1} = y_k + \\sigma \\left(A\\bar{x}_{k+1} - b\\right)$ |Dual Forward-Backward step    |\n",
    "|$x_{k+1} = \\bar{x}_{k+1} - \\tau A^T\\left(\\bar{y}_{k+1} - y_k\\right)$ |Primal Extrapolation step   |\n",
    "|$y_{k+1} = \\bar{y}_{k+1}$               |Dual Extrapolation step        |\n",
    "\n",
    "|        Version 2       |       Interpretation        |\n",
    "|:----------------------:|:---------------------------:|\n",
    "| $\\bar{y}_{k+1} = y_k + \\sigma \\left(Ax_{k} - b\\right)$ |    Dual Forward-Backward step    |\n",
    "| $\\bar{x}_{k+1} = \\mathrm{prox}_{\\tau f}\\left(x_k - \\tau A^T\\bar{y}_{k+1}\\right)$ |  Primal Forward-Backward step  |\n",
    "| $y_{k+1} = \\bar{y}_{k+1} + \\sigma A\\left(\\bar{x}_{k+1} - x_k\\right)$ |    Dual Extrapolation step    |\n",
    "|               $x_{k+1} = \\bar{x}_{k+1}$               |      Primal Extrapolation step      |\n",
    "\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6\n",
    "\n",
    "*Basis Pursuit* is mathematically formulated as: \n",
    "\n",
    "\\begin{equation}\n",
    "        \\min_{x \\in \\mathbb{R}^n} ~ \\|x\\|_1 \\hspace{1cm} \\text{subject to} \\hspace{1cm}  Ax = b\n",
    "\\end{equation}\n",
    "\n",
    "Here, we set $n = 20$ and $m = 10$, and generate an i.i.d. Gaussian matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PDHG import PDHG2\n",
    "from OptimizationProblems import BasisPursuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "n, m = 20, 10\n",
    "np.random.seed(6)\n",
    "A = np.random.randn(m, n)\n",
    "b = np.random.randn(m)\n",
    "\n",
    "# Problem initialization.\n",
    "prob = BasisPursuit(n, m, A, b)\n",
    "x_star, f_star, L, gamma = prob.prob_parameters(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDHG version 1 solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values \n",
    "xo = np.zeros(n)\n",
    "yo = np.random.rand(m)\n",
    "\n",
    "Alg = PDHG(xo, yo, prob.prox_L1N, A, b, x_star)\n",
    "result = Alg.two_runs(stop_crt_kwargs={'f':prob.L1N, 'eta_fun':prob.eta_QEBSG})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDHG version 2 solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alg2 = PDHG2(xo, yo, prob.prox_L1N, A, b, x_star)\n",
    "result2 = Alg2.two_runs(stop_crt_kwargs={'f':prob.L1N, 'eta_fun':prob.eta_QEBSG}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measures computed at the solutions of both versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1 \n",
    "bounds = OptimizationBounds(result[\"Primal variables\"], result[\"Dual variables\"], prob.L1N, prob.prox_L1N, \n",
    "            prob.stationarity, prob.L1Nc, prob.proj_L1Nc, f_star, x_star, A, b, \n",
    "            L=L, Lc=0, eta_fun= prob.ETA_BETAs, gamma=gamma)\n",
    "\n",
    "# Version 2\n",
    "bounds2 = OptimizationBounds(result2[\"Primal variables\"], result2[\"Dual variables\"], prob.L1N, prob.prox_L1N, \n",
    "            prob.stationarity, prob.L1Nc, prob.proj_L1Nc, f_star, x_star, A, b, \n",
    "            L=L, Lc=0, eta_fun=prob.ETA_BETAs, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OG, FG, KKT, PDG, SDG = bounds.OG, bounds.FG, bounds.KKT, bounds.PDG, bounds.SDG[:, -1]\n",
    "OG2, FG2, KKT2, PDG2, SDG2 = bounds2.OG, bounds2.FG, bounds2.KKT, bounds2.PDG, bounds2.SDG[:, -1]\n",
    "min_ite = min(len(KKT), len(KKT2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bounds computed at the solution of version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OG_bounds, KKT_SDG_bounds, PDG_SDG_bounds = bounds2.all_bounds()\n",
    "\n",
    "exp_name = 'Basis Pursuit'\n",
    "OG_all[exp_name] = (OG_bounds, result['step'])\n",
    "\n",
    "comparability = {GK: [KKT_SDG_bounds['G<K']['SDG'], KKT_SDG_bounds['G<K']['bound']],\n",
    "                 KG: [KKT2, KKT_SDG_bounds['K<G']],\n",
    "                 GD: [PDG_SDG_bounds['G<D']['SDG'], PDG_SDG_bounds['G<D']['bound']],\n",
    "                 DG: [PDG2, PDG_SDG_bounds['D<G']]}\n",
    "comparability_all[exp_name] = comparability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = Plotter()\n",
    "# plots.SDG_stability(KKT[:min_ite], KKT2[:min_ite], SDG[:min_ite], SDG2[:min_ite], result['step'])\n",
    "# plots.findings_plot(OG2, FG2, KKT2, PDG2, SDG2, OG_bounds, KKT_SDG_bounds, PDG_SDG_bounds, result['step'])\n",
    "plots.plot_all(paper=False, prob=exp_name, OG=OG2, FG=FG2, KKT=KKT, KKT2=KKT2, PDG=PDG2, SDG = SDG, SDG2=SDG2, OG_bounds=OG_bounds, KKT_SDG_bounds=KKT_SDG_bounds, PDG_SDG_bounds=PDG_SDG_bounds, step=result['step'], min_ite=min_ite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Comparison \n",
    "\n",
    "Lastly, we conduct a benchmark analysis by applying our findings to the six experiments previously discussed.\n",
    "\n",
    "* Initially, we report the number of iterations required by each problem to identify an $\\varepsilon = 10^{−7}$ solution, employing various stopping criteria. \n",
    "\n",
    "    Note that, the same data is used to compute the 3 measures (*KKT error*, *SDG*, and *PDG*) for each experiment. It provides additional evidence, showing that the smoothed duality gap consistently achieves $\\varepsilon$-solutions with fewer iterations across nearly all of our experiments, thus demonstrating its superior efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OG_table = find_iter(OG_all, 1e-7)\n",
    "OG_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Subsequently, we demonstrate the tightness of our comparability bounds, as outlined in Section **4**, across each experiment. This is achieved by presenting the average and standard deviation of the ratio of each result, more specifically, for the bound $\\mathcal{M}_1 \\leq \\mathcal{W}(\\mathcal{M}_2)$. The displayed values represent the average and standard deviation of the term $\\frac{\\mathcal{W}(\\mathcal{M}_2)}{\\mathcal{M}_1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparability_table = avg_sd(comparability_all)\n",
    "comparability_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
